# -*- coding: utf-8 -*-
"""gradients_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17feC9ybeHEBWxtFLcAT4rE2r2WRE-Frn
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import matplotlib.pyplot as plt
# %matplotlib inline


def train():
    batch = 1000
    input_dimension = 10
    output_dimension = 3
    model_input = tf.random.uniform(
        shape=[batch, input_dimension], minval=0, maxval=3, dtype=tf.float32)
    labels = tf.one_hot(tf.random.uniform(
        shape=[batch], minval=0, maxval=3, dtype=tf.int32), depth=output_dimension)

    # init = tf.keras.initializers.RandomNormal(stddev=0.01)
    # init = tf.keras.initializers.glorot_normal()
    #init = tf.keras.initializers.glorot_uniform()
    #init = tf.keras.initializers.he_normal()
    init = tf.keras.initializers.he_uniform()
    full_connection1 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    full_connection2 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    full_connection3 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    full_connection4 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    full_connection5 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    full_connection6 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    full_connection7 = tf.keras.layers.Dense(
        100, activation=None, kernel_initializer=init)
    output = tf.keras.layers.Dense(
        output_dimension, activation=None, kernel_initializer=init)

    def model(model_input):
        activ = tf.nn.relu
        with tf.GradientTape() as tape:
            fc1 = full_connection1(model_input)
            fc1_activation = tf.keras.layers.Activation(activ)(fc1)
            fc2 = full_connection2(fc1_activation)
            fc2_activation = tf.keras.layers.Activation(activ)(fc2)
            fc3 = full_connection3(fc2_activation)
            fc3_activation = tf.keras.layers.Activation(activ)(fc3)
            fc4 = full_connection4(fc3_activation)
            fc4_activation = tf.keras.layers.Activation(activ)(fc4)
            fc5 = full_connection5(fc4_activation)
            fc5_activation = tf.keras.layers.Activation(activ)(fc5)
            fc6 = full_connection6(fc5_activation)
            fc6_activation = tf.keras.layers.Activation(activ)(fc6)
            fc7 = full_connection7(fc6_activation)
            fc7_activation = tf.keras.layers.Activation(activ)(fc7)
            model = output(fc7_activation)

            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
                logits=model, labels=labels))

        return tape.gradient(loss, fc1)
    obj = model(model_input)
    print(obj)
    if obj == None:
        obj = tf.zeros(shape=[10000])
    else:
        obj = tf.reshape(obj, shape=[-1, ])

    plt.plot(obj)
    plt.ylim(-0.001, 0.001)
    plt.show()


if __name__ == "__main__":
    train()
